---
title: "Notes on the C Version of Smacof"
author: 
- Jan de Leeuw - University of California Los Angeles
date: '`r paste("Started October 10 2023, Version of",format(Sys.Date(),"%B %d, %Y"))`'
output:
  bookdown::pdf_document2:
    latex_engine: lualatex
    includes:
      in_header: preamble.tex
    keep_tex: yes
    toc: true
    toc_depth: 4
    number_sections: yes
  bookdown::html_document2:
    keep_md: yes
    css: preamble.css
    toc: true
    toc_depth: 4
    number_sections: yes
graphics: yes
mainfont: Times New Roman
fontsize: 12pt
bibliography: ["mypubs.bib","total.bib"]
abstract: TBD
---

```{r loadpackages, echo = FALSE}
#suppressPackageStartupMessages (library (foo, quietly = TRUE))
```

```{r load code, echo = FALSE}
#source("rcode/smacofSort.R")
```

**Note:** This is a working paper which will be expanded/updated frequently. All suggestions for improvement are welcome. All Rmd, tex, html, pdf, R, and C files are in the public domain. Attribution
will be appreciated, but is not required. The files can be found at
https://github.com/deleeuw/mdsStruct. 

# Introduction

The loss function in (metric, least squares, Euclidean, symmetric) Multidimensional Scaling (MDS)
is
$$
\sigma(X):=\frac12\jis w_{ij}(\delta_{ij}-d_{ij}(X))^2.
$$

This assumes symmetry and it uses all elements below the diagonal of both $W$, $\Delta$, and $D(X)$.
For missing data we set $w_{ij}=0$. 


# Example

Here is a small input example.

# Unweighted, full matrix

## Unnormalized 

$W=E-I$ and $V=(I-E)+(n-1)I=nI-E=nJ$. Thus $V^+=n^{-1}J$ and $V^+B=n^{-1}B$.

## Normalized 

After normalization $W=\frac{1}{n(n-1)}(E-I)$ and $\sum\sum w_{ij}\delta_{ij}{^2}=1$
or $\sum\sum\delta_{ij}{^2}=n(n-1)$. Also $V=\frac{1}{n(n-1)}(nI-E)=\frac{1}{(n-1)}J$, which means $V^+=(n-1)J$. For
$$
B(X)=\sum_{i<j}w_{ij}\frac{\delta_{ij}}{d_{ij}(X)}A_{ij}=\frac{1}{n(n-1)}\sum_{i<j}\frac{\delta_{ij}}{d_{ij}(X)}A_{ij}
$$
and thus 
$$
V^+B(X)=\frac{1}{n}\sum_{i<j}\frac{\delta_{ij}}{d_{ij}(X)}A_{ij}
$$

# Unweighting

\begin{align}
\sigma(d)&=\sum_{k=1}^Kw_k(\delta_k-d_k)^2\\
&=\sum_{k=1}^Kw_k(\delta_k-\overline{d}_k)^2-2\sum_{k=1}^Kw_k(\delta_k-\overline{d}_k)(d_k-\overline{d}_k)+\sum_{k=1}^Kw_k(d_k-\overline{d}_k)^2.
\end{align}
Now suppose $w_k\leq w_\star$ and define
$$
r_k=\frac{w_k}{w_\star}(\delta_k-\overline{d}_k)
$$
\begin{align*}
\sigma(d)&\leq\sigma(\overline{d})+w_\star\left\{\sum_{k=1}^K(d_k-\overline{d}_k)^2-\sum_{k=1}^Kr_k(d_k-\overline{d}_k)\right\}\\&=\sigma(\overline{d})+w_\star\left\{\sum_{k=1}^K(d_k-(\overline{d}_k+r_k))^2-\sum_{k=1}^Kr_k^2\right\}.
\end{align*}
Note 
$$
\delta_k(X):=\frac{w_k}{w_\star}\delta_k+(1-\frac{w_k}{w_\star})d_k(X)
$$
So given $X$ computed the adjusted dissimilarities $\Delta(X)$ and perform one or more unweighted smacof
steps to compute the update $X^+$. Of course if all $w_k$ are the same then $\Delta(X)=\Delta$ and we
compute a regular unweighted smacof. It is of some interest to study how many smacof steps to make
before computing a new $\Delta(X)$.

# Thoughts on ALS

In nonmetric MDS we minimize
$$
\sigma(\Delta,X)=\sum_{k=1}^K w_k(\delta_k-d_k(X))^2
$$
over the configurations $X$ and the transformed dissimilarities $\Delta$,
where we assume $\Delta\in\mathfrak{D}$, with $\mathfrak{D}$ the intersection of
a convex cone and a sphere.

## The Single-Step approach

@kruskal_64a, @kruskal_64b defines
$$
\sigma_\star(X):=\min_{\Delta\in\mathfrak{D}}\sigma(\Delta,X)
$$
and
$$
\Delta(X)=\mathop{\text{argmin}}_{\Delta\in\mathfrak{D}}\sigma(\Delta,X)
$$
Thus
$$
\sigma_\star(X)=\sigma(\Delta(X),X)
$$
which is now a function of $X$ only. Under some conditions, which are usually true in MDS,
$$
\mathcal{D}\sigma_\star(X)=\mathcal{D}_2\sigma(\Delta(X),X)
$$
where $\mathcal{D}_2\sigma(\Delta(X),X)$ are the partials of $\sigma$ with respect to $X$.
Thus the partials of $\sigma_\star$ can be computed by evaluating the partials of $\sigma$ 
with repesct to $X$ at $(X,\Delta(X))$. This has created much confusion in the past. We can now solve the problem of minimizing $\sigma_\star$, which is a function of $X$ alone. 

I think Guttman calls this the *single step approach*. A variation of Kruskal's single-step approach defines
$$
\sigma_G(X)=\sum_{k=1}^Kw_k(\delta_k^\#(X)-d_k(X))^2
$$
where the $\delta_k^\#(X)$ are *Guttman's rank images*, i.e. the permutation of the
$d_k(X)$ that makes it monotone with the $\delta_k$ (@guttman_68). Or, alternatively, we define
$$
\sigma_S(X):=\sum_{k=1}^Kw_k(\delta_k^\%(X)-d_k(X))^2
$$
where the $\delta_k^\%(X)$ are *Shepard's rank images*, i.e. the permutation of
the $\delta_k$ that makes it monotone with the $d_k(X)$ (@shepard_62a, @shepard_62b).

The Shepard and Guttman alternatives are computationally more intricate and more complicated
than the Kruskal *monotone regression* approach, mostly because of problems with uniqueness and differentiation, but they are obviously both single step approaches.

## The Two-step Approach

The *two-step approach* or *alternating least squares* approach alternates minimization
of $\sigma(\Delta,X)$ over $X$ for our current best estimate of $\Delta$ with
minimization of $\sigma(\Delta,X)$ over $\Delta\in\mathfrak{D}$ for our current best
value of $X$. Thus an update looks like
\begin{align}
\Delta^{(k)}&=\mathop{\text{argmin}}_\Delta\sigma(\Delta,X^{(k)}),\\
X^{(k+1)}&=\mathop{\text{argmin}}_X\sigma(\Delta^{(k)},X).
\end{align}
This approach to MDS was in the air since the early (unsuccessful) attempts around 1968 of Young and De Leeuw to combine Torgerson's classic metric MDS method with Kruskal's monotone regression transformation.

As formulated, however, there are some problems with the ALS algorithm.
Step \@ref(eq:step1) is easy to carry out, using monotone regression. Step \@ref(eq:step2) means solving a metric scaling problem,
which is an iterative proces that requires an infinite number of iterations. Thus, what is usually
implemented, is to combine step \@ref(eq:step1) with one of more iterations of a convergent iterative procedure
for metric MDS, such as smacof. If we take only one of these *inner iterations* the algorithm
becomes indistinguishable from Kruskal's single step method. This has also created much confusion in the
past.

It is somewhat worrisome that in the ALS approach we solve the first subproblem \@ref(eq:step1) exactly, while we take only a single step towards the solution for given $\Delta$ in the second subproblem \@ref(eq:step2). If we have an
infinite iterative procedure to compute the optial $\Delta\in\mathfrak{D}$ for given $X$, then
a more balanced approach is to take several inner iterations in the first step and several
innewr iterations in the second step. How many of each, nobody knows.

# CCD

Minimize
$$
\sigma(\beta,X)=\mathop{\sum\sum}_{1\leq i<j\leq n}w_{ij}(\delta_{ij}(\beta)-d_{ij}(X))^2=(Z\beta-d)'W(Z\beta-d)
$$
$\beta^+=\beta+\epsilon\ e_j$ and $Z\beta^+=Z\beta+\epsilon z_j$
$$
\sigma(\beta^+, X)=((Z\beta-d)+\epsilon z_j)'W((Z\beta-d)+\epsilon z_j)=\sigma(\beta,X)+2\epsilon z_j'Wr+\epsilon^2z_j'Wz_j
$$
$$
\epsilon^+=\frac{r'Wz_j}{z_j'Wz_j}
$$
(in the ordinal case $\epsilon^+=0$ if $r'Wz_j<0$). Now update $\beta$ and update
$r$ wih $r^+=Z\beta^+-d=r+\epsilon^+ z_j$.

# Memory

This is mostly for my own education.

```{c memory, eval = FALSE}
#include <stdio.h>
#include <stdlib.h>

int main(void) {
  int n = 10, p = 4;
  char *a = NULL;
  double **x = (double **)calloc((size_t)n, sizeof(double *));
  double y[10][4] = {{0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0},
                     {0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0}, {0, 0, 0, 0},
                     {0, 0, 0, 0}, {0, 0, 0, 0}};
  for (int i = 0; i < n; i++) {
    x[i] = (double *)calloc((size_t)p, sizeof(double));
  }
  printf("x[i][j] (dynamic allocation on heap)\n");
  for (int i = 0; i < n; i++) {
    for (int j = 0; j < p; j++) {
      printf("%ld ", (uintptr_t)&x[i][j]);
    }
    printf("\n");
  }
  printf("\n\n");
  printf("x[i] and &x[i] and sizeof x[i]\n");
  for (int i = 0; i < n; i++) {
    printf("%ld %ld %ld\n", (uintptr_t)x[i], (uintptr_t)&x[i], sizeof x[i]);
  }
  printf("\n\n");
  printf("x\n");
  printf("%ld\n", (uintptr_t)x);
  printf("\n\n");
  printf("*******************************************\n");
  printf("\n\n");
  printf("y[i][j] (static allocation on stack)\n");
  for (int i = 0; i < n; i++) {
    for (int j = 0; j < p; j++) {
      printf("%ld ", (uintptr_t)&y[i][j]);
    }
    printf("\n");
  }
  printf("\n\n");
  printf("y[i] and &y[i] and sizeof y[i]\n");
  for (int i = 0; i < n; i++) {
    printf("%ld %ld %ld\n", (uintptr_t)y[i], (uintptr_t)&y[i], sizeof y[i]);
  }
  printf("\n\n");
  printf("y\n");
  printf("%ld\n", (uintptr_t)y);
  printf("\n\n");
  for (int i = 0; i < n; i++) {
    free(x[i]);
  }
  free(x);
  return EXIT_SUCCESS;
}
```

# Hessian

There are several ways to think of the Hessian. The simplest one (perhaps) is
as an $np\times np$ symmetric matrix (corresponding to column-major R vector
of length $\frac12 np(np+1)$). This is what we would use for a straightforward
version of Newton-Raphson.


It is more elegant, however, to think of
$H$ as a symmetric super-matrix of order $p$, with as elements $n\times n$
matrices. And, for some purposes, such as the pseudo-confidence ellipsoids in @deleeuw_E_17q, as 
a super-matrix of order $n$ with as elements $p\times p$ matrices. Both the
super-matrix interpretations lead to four-dimensional arrays, the first
a $p\times p\times n\times n$ array, the second an $n\times n\times p\times p$
array. The different interpretations lead to different ways to store the
Hessian in memory, and to different ways to retrieve its elements. Of course
we can write routines to transform from one interpretation to another.

# Appendix: Code

We use *qsort* to sort the rows of the input data frame by increasing delta. The sorting
is done in C, the R version is a wrapper around the compiled C code. The C code also
contains a main which analyzes the same small example as we have used in the text.
Compile with "clang -o runner -O2 smacofSort.c" and then start "runner" in the shell.
The C code uses the .C() interface in R, which can be improved using .Call(), 
but probably in this case with little gain.

# smacofMaximumSum

Maximize
$$
\jis w_{ij}\delta_{ij}^2d_{ij}^2(X)
$$
over $X$ with $\text{tr}\ (X'X)^2=1$. This gives $BX=X\Lambda$, with
$$
B=\jis w_{ij}\delta_{ij}^2A_{ij}
$$
There is also a non-metric version. Maximize over $\text{tr}\ (X'X)^2$
$$
\jis\mathop{\sum\sum}_{1\leq k<l\leq n} w_{ij,kl}\text{sign}(\delta_{ij}-\delta_{kl})(d_{ij}^2(X)-d_{kl}^2(X))
$$
which simplifies to
$$
2\ \jis d_{ij}^2(X)\mathop{\sum\sum}_{1\leq k<l\leq n}w_{ij,kl}\text{sign}(\delta_{ij}-\delta_{kl})
$$
Simplifies more $w_{ij,kl}=w_{ij}$ or $_{ij,kl}=w_{ij}w_{kl}$

Also note
$$
\rho(X)=\jis w_{ij}\delta_{ij}d_{ij}(X)=
\jis \frac{w_{ij}}{\delta_{ij}d_{ij}(X)}\delta_{ij}^2d_{ij}^2(X)\approx\jis
\frac{w_{ij}}{\delta_{ij}^2}\delta_{ij}^2d_{ij}^2(X)=\eta^2(X)
$$

# smacofElegant

# smacofAdjustDiagonal

# smacofImpute

# smacofHildreth



@hildreth_57

Consider the QP problem of minimizing $f(x)=\frac12(x-y)'W(x-y)$
over all $x\in\mathbb{R}^n$ satisfying $Ax\geq 0$, where $A$ is $m\times n$. Wlg we can assume $a_j'Wa_j=1$. The Lagrangian is
$$
\mathcal{L}(x,\lambda)=\frac12(x-y)'W(x-y)-\lambda'Ax.
$$
$$
\max_{\lambda\geq 0}\mathcal{L}(x,\lambda)=\begin{cases}\frac12(x-y)'W(x-y)&\text{ if }Ax\geq 0,\\
+\infty&\text{ otherwise}.\end{cases}
$$
and thus
$$
\min_{x\in\mathbb{R}^n}\max_{\lambda\geq 0}\mathcal{L}(x,\lambda)=\min_{Ax\geq 0}\frac12(x-y)'W(x-y).
$$
By duality
$$
\min_{x\in\mathbb{R}^n}\max_{\lambda\geq 0}\mathcal{L}(x,\lambda)=\max_{\lambda\geq 0}\min_{x\in\mathbb{R}^n}\mathcal{L}(x,\lambda).
$$
The inner minimum over $x$ is attained for 
$$
x=y+W^{-1}A'\lambda,
$$ 
and is equal to
$$
\min_{x\in\mathbb{R}^n}\mathcal{L}(x,\lambda)=-\frac12\lambda'AW^{-1}A'\lambda-\lambda'Ay.
$$
Thus
$$
\max_{\lambda\geq 0}\min_{x\in\mathbb{R}^n}\mathcal{L}(x,\lambda)=
-\frac12\min_{\lambda\geq 0}\left\{(Wy+A'\lambda)'W^{-1}(Wy+A'\lambda)-y'Wy\right\}
$$
We minimize $h(\lambda)=(Wy+A'\lambda)'W^{-1}(Wy+A'\lambda)$ with coordinate descent. Let $\lambda_j(\eps)=\lambda+\eps e_j$. Then
$$
h(\lambda_j(\eps))=(Wy+A'\lambda+\eps a_j)'W^{-1}()=\eps^2a_j'W^{-1}a_j+2\eps a_j'W^{-1}(y+W^{-1}A'\lambda)+
$$
which must be minimized over $\eps\geq-\lambda_j$. So the minimum is attained at
$$
\eps=-\frac{a_j'W^{-1}x}{a_j'W^{-1}a_j}
$$
with $x=y+W^{-1}A'\lambda$ (cf ...), provided ... satisfies .. Otherwise $\eps=-\lambda_j$. Now update both $\lambda$ and $x$,
and go to the next $j$.

# smacofDykstra

# smacofJacobi

taken from @deleeuw_E_17o

partial jacobi

# smacofIndividualDifferenceModels.c

\begin{align}
X_k&=X,\\
X_k&=X\Lambda_k \text{ with }\Lambda_k\text{ diagonal},\\
X_k&=XC_k,\\
X_k&=X\Lambda_k Y' \text{ with }\Lambda_k\text{ diagonal},\\
X_k&=XC_kY',\\
X_k&=X\Lambda_k Y_k'\text{ with }\Lambda_k\text{ diagonal}\\
X_k&=XC_k Y'\text{ with } C_k=\sum_{s=1}^r z_{ks}H_s.
\end{align}

# smacofSort


# Code

## smacofSort.R


## smacofSort.c

```{c file_auxilary2, code = readLines(""), eval = FALSE}
```

# References